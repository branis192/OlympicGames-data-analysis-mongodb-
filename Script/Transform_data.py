# -*- coding: utf-8 -*-
"""import_Data_Mongodp_Files.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AWqJinXbMhKjbRgkm5zem73z7THJ9wmn
"""

!unzip "/content/Partie 1(2).zip"

!unzip "/content/World_Athletic_Championships.zip"

import pandas as pd
import glob
import os
import json

def process_data():
    # --- CONFIGURATION ---
    results_folder = "/content/World_Athletic_Championships/results"  # Dossier contenant les 811 fichiers
    index_csv = "/content/World_Athletic_Championships/World_Athletic_Championships.csv"

    output_results_json = "world_results.json"
    output_index_json = "championships_index.json"

    # --- 1. TRAITEMENT DES 811 FICHIERS DE R√âSULTATS ---
    all_results = []
    csv_files = glob.glob(os.path.join(results_folder, "*.csv"))

    print(f"üîÑ Phase 1 : Conversion de {len(csv_files)} fichiers de r√©sultats...")

    for file in csv_files:
        try:
            # Lecture du CSV
            df = pd.read_csv(file)

            # Correction automatique de la faute de frappe 'athelete' -> 'athlete'
            if 'athelete' in df.columns:
                df = df.rename(columns={'athelete': 'athlete'})

            # Nettoyage des espaces vides dans les noms de colonnes
            df.columns = [c.strip() for c in df.columns]

            # Conversion en dictionnaire et ajout √† la liste globale
            all_results.extend(df.to_dict(orient='records'))
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur sur le fichier {file}: {e}")

    # Sauvegarde des r√©sultats en JSON
    with open(output_results_json, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=4, ensure_ascii=False)
    print(f"‚úÖ Fichier '{output_results_json}' g√©n√©r√©.")

    # --- 2. TRAITEMENT DU FICHIER D'INDEX (CHAMPIONNATS) ---
    print(f"üîÑ Phase 2 : Conversion de l'index '{index_csv}'...")
    try:
        # On utilise le s√©parateur ';' car ton fichier est structur√© ainsi
        df_index = pd.read_csv(index_csv, sep=";")

        # Nettoyage des noms de colonnes (minuscules, pas d'espaces)
        df_index.columns = [c.strip().replace(' ', '_').lower() for c in df_index.columns]

        # Sauvegarde en JSON
        df_index.to_json(output_index_json, orient="records", indent=4, force_ascii=False)
        print(f"‚úÖ Fichier '{output_index_json}' g√©n√©r√©.")

    except Exception as e:
        print(f"‚ùå Erreur lors du traitement de l'index : {e}")

    print("\nüöÄ TERMIN√â ! Les fichiers JSON sont pr√™ts pour l'importation.")

if __name__ == "__main__":
    process_data()

import pandas as pd
import json
import os

# Chemins des fichiers (√† adapter selon ton dossier)
PATH_OLYMPEDIA = "./olympedia/"
PATH_WORLDS = "./world_championships/"

# 1. Charger la Bio des athl√®tes
bio_df = pd.read_csv(os.path.join(PATH_OLYMPEDIA, 'Olympic_Athlete_Bio.csv'))

# Dictionnaire pour stocker les athl√®tes en m√©moire et calculer les compteurs
athletes_map = {}
events_map = {}
editions_map = {}

for _, row in bio_df.iterrows():
    a_id = str(row['athlete_id'])
    athletes_map[a_id] = {
        "_id": a_id,
        "name": row['name'],
        "sex": row['sex'],
        "born": str(row['born']),
        "height": row['height'],
        "weight": row['weight'],
        "country_origin": row['country_noc'] if 'country_noc' in row else "",
        "total_medals": 0,
        "medals_detail": {"Gold": 0, "Silver": 0, "Bronze": 0}
    }

# Cette fonction traite une ligne de r√©sultat (JO ou Mondial), d√©normalise les donn√©es (sexe, ann√©e) et met √† jour les collections events et editions.
results_to_import = []
import re

def process_row(row, comp_name):
    # 1. Extraction de l'ann√©e depuis la colonne 'edition' (ex: "1908 Summer Olympics")
    # Si 'year' n'existe pas, on le cherche dans 'edition'
    if 'year' in row and pd.notna(row['year']):
        year = int(row['year'])
    else:
        edition_str = str(row.get('edition', ''))
        match = re.search(r'\d{4}', edition_str)
        year = int(match.group()) if match else 0

    # 2. Gestion du NOM et de l'ID
    # Dans ton fichier c'est 'athlete' et 'athlete_id'
    name = row.get('athlete', row.get('name', row.get('Name', 'Unknown')))
    a_id = str(row.get('athlete_id', name))

    # 3. R√©cup√©ration des autres infos (avec tes noms de colonnes r√©els)
    event_name = row.get('event', 'Unknown')
    medal_val = row.get('medal', "na")
    medal = str(medal_val).strip() if pd.notna(medal_val) else "na"
    noc = row.get('country_noc', row.get('noc', 'Unknown'))
    pos = str(row.get('pos', 'na'))

    # 4. R√©cup√©ration du sexe via la bio (charg√©e au d√©but)
    sex = athletes_map.get(a_id, {}).get('sex', 'Unknown')

    # Cr√©ation du document pour la collection 'results'
    res_doc = {
        "athlete_id": a_id,
        "athlete_name": name,
        "sex": sex,
        "year": year,
        "competition": comp_name,
        "event": event_name,
        "pos": pos,
        "medal": medal,
        "noc": noc
    }
    results_to_import.append(res_doc)

    # --- MISE √Ä JOUR DES COMPTEURS POUR LES AUTRES COLLECTIONS ---

    # Mise √† jour 'athletes' (total_medals)
    if medal in ["Gold", "Silver", "Bronze"]:
        if a_id in athletes_map:
            athletes_map[a_id]["total_medals"] += 1
            athletes_map[a_id]["medals_detail"][medal] += 1

    # Mise √† jour 'events' (nb_editions)
    event_key = f"{event_name}_{sex}".lower().replace(" ", "_")
    if event_key not in events_map:
        events_map[event_key] = {"_id": event_key, "event_name": event_name, "gender": sex, "years": set()}
    events_map[event_key]["years"].add(year)

    # Mise √† jour 'editions' (count_disciplines)
    # On utilise l'√©dition compl√®te comme ID (ex: "1908 Summer Olympics")
    edit_id = str(row.get('edition', year))
    if edit_id not in editions_map:
        editions_map[edit_id] = {"_id": edit_id, "year": year, "competition": comp_name, "disciplines": set()}
    editions_map[edit_id]["disciplines"].add(event_name)

# Traitement du gros fichier de r√©sultats JO
results_jo = pd.read_csv(os.path.join(PATH_OLYMPEDIA, 'Olympic_Athlete_Event_Results.csv'))
for _, row in results_jo.iterrows():
    process_row(row, "Olympics")

# Traitement des 811 fichiers Mondiaux
for i in range(811):
    file_path = os.path.join(PATH_WORLDS, f"{i}.csv")
    if os.path.exists(file_path):
        df_world = pd.read_csv(file_path)
        for _, row in df_world.iterrows():
            process_row(row, "World Championships")

# Finalisation des champs calcul√©s pour 'events' et 'editions'
final_events = []
for k, v in events_map.items():
    v["nb_editions"] = len(v["years"])
    v["years_active"] = sorted(list(v["years"]))
    del v["years"] # On supprime le set temporaire
    final_events.append(v)

final_editions = []
for k, v in editions_map.items():
    v["count_disciplines"] = len(v["disciplines"])
    del v["disciplines"]
    final_editions.append(v)

def save_json(data, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False)

save_json(list(athletes_map.values()), "athletes.json")
save_json(results_to_import, "results.json")
save_json(final_events, "events.json")
save_json(final_editions, "editions.json")

print("Fichiers JSON pr√™ts pour l'importation !")

# !zip -r [nom_du_fichier_final].zip [chemin_du_dossier_a_compresser]
!zip -r dossier_JSON_MONGODB.zip /content/fichier_JSON

